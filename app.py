import requests, time, os, shutil, schedule, json, re
from selenium import webdriver
from selenium.webdriver import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.firefox.options import Options
from datetime import datetime, timedelta
from dotenv import load_dotenv
from selenium.webdriver.common.action_chains import ActionChains
import logging
import sys
from pathlib import Path
from selenium.common.exceptions import ElementClickInterceptedException

# ‚úÖ NOVO (Fase 5): Importar SQLAlchemy ORM do package models
from models import insert_records_sqlalchemy

# Criar pasta de logs se n√£o existir
logs_dir = os.path.join(os.path.dirname(__file__), 'logs')
os.makedirs(logs_dir, exist_ok=True)

# Configura√ß√£o de logging
logging.basicConfig(
    filename=os.path.join(logs_dir, 'robo_download.log'),
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)
# Adicionar handler para logar no console tamb√©m
console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
console.setFormatter(formatter)
logger.addHandler(console)

# Fun√ß√£o para notifica√ß√£o (placeholder)
def send_notification(msg):
    # Aqui voc√™ pode implementar envio por e-mail, Telegram, etc.
    logger.warning(f'NOTIFICA√á√ÉO: {msg}')

# Carrega as vari√°veis do arquivo .env
if not Path('.env').exists():
    print('Arquivo .env n√£o encontrado!')
    sys.exit(1)
load_dotenv('.env', override=True)

# Fun√ß√£o para checar vari√°veis obrigat√≥rias
def checar_variaveis_obrigatorias(vars_list):
    faltando = [v for v in vars_list if not os.getenv(v)]
    if faltando:
        print(f"Vari√°veis obrigat√≥rias faltando no .env: {', '.join(faltando)}")
        sys.exit(1)

checar_variaveis_obrigatorias([
    'SYS_URL', 'SYS_USERNAME', 'SYS_PASSWORD', 'SYS_SECRET_OTP', 'DESTINO_FINAL_DIR',
    'BROWSER', 'RETRIES_DOWNLOAD', 'TIMEOUT_DOWNLOAD', 'HEADLESS', 'OTP_URL',
    'DB_SERVER', 'DB_DATABASE', 'DB_USERNAME', 'DB_PASSWORD', 'DB_DRIVER'
])

# Timeouts configur√°veis
TIMEOUT_DOWNLOAD = int(os.getenv('TIMEOUT_DOWNLOAD', '60'))
RETRIES_DOWNLOAD = int(os.getenv('RETRIES_DOWNLOAD', '3'))
DOWNLOAD_RETRY_DELAY = int(os.getenv('DOWNLOAD_RETRY_DELAY', '120'))  # ‚úÖ NOVO: Delay entre tentativas (padr√£o: 120s)

# Diret√≥rio de downloads - usar pasta do projeto por padr√£o
DOWNLOADS_DIR = os.getenv('DOWNLOADS_DIR', os.path.join(os.path.dirname(__file__), 'downloads'))
os.makedirs(DOWNLOADS_DIR, exist_ok=True)

# Carrega os XPaths do arquivo map_relative.json (XPaths relativos mais robustos)
# Sempre carrega de \bases\
map_relative_file = os.path.join(os.path.dirname(__file__), 'bases', 'map_relative.json')
if not os.path.exists(map_relative_file):
    print(f"Arquivo map_relative.json n√£o encontrado em: {map_relative_file}")
    sys.exit(1)

with open(map_relative_file, 'r', encoding='utf-8') as f:
    XPATHS = json.load(f)

logger.info(f"Mapeamento de XPaths carregado de: {map_relative_file}")

# Acessando as vari√°veis
url = os.getenv("SYS_URL")
username = os.getenv("SYS_USERNAME")
password = os.getenv("SYS_PASSWORD")
secret_otp = os.getenv("SYS_SECRET_OTP")
destino_final_dir = os.getenv("DESTINO_FINAL_DIR")
browser = os.getenv("BROWSER")
print(f"Valor lido de BROWSER no .env: {browser!r}")
browser = (browser or "").strip().replace('"','').replace("'","").lower()
headless = os.getenv("HEADLESS", "false").lower() == "true"
otp_url = os.getenv("OTP_URL", "http://localhost:8000/generate_otp")

# Novas configura√ß√µes para envio
DRY_RUN = os.getenv('DRY_RUN', 'false').lower() == 'true'
BATCH_SIZE = int(os.getenv('BATCH_SIZE', '25'))
POST_RETRIES = int(os.getenv('POST_RETRIES', '3'))
BACKOFF_BASE = float(os.getenv('BACKOFF_BASE', '1.5'))

logger.info(f"DRY_RUN={DRY_RUN} BATCH_SIZE={BATCH_SIZE} POST_RETRIES={POST_RETRIES} BACKOFF_BASE={BACKOFF_BASE}")

logger.info(f"Valor de url: {url!r}")

def iniciar_driver():
    logger.info(f"Iniciando driver do navegador... ({browser})")
    logger.info(f"Downloads ser√£o salvos em: {DOWNLOADS_DIR}")
    
    driver = None
    if browser == "chrome":
        from selenium.webdriver.chrome.options import Options as ChromeOptions
        options = ChromeOptions()
        # options.add_argument("--start-maximized")
        if headless:
            options.add_argument("--headless=new")
        options.add_argument("--log-level=3")
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        prefs = {
            "download.default_directory": DOWNLOADS_DIR,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True
        }
        options.add_experimental_option("prefs", prefs)
        driver = webdriver.Chrome(options=options)
    elif browser == "edge":
        from selenium.webdriver.edge.options import Options as EdgeOptions
        options = EdgeOptions()
        options.add_argument("--start-maximized")
        if headless:
            options.add_argument("--headless=new")
        prefs = {
            "download.default_directory": DOWNLOADS_DIR,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True
        }
        options.add_experimental_option("prefs", prefs)
        driver = webdriver.Edge(options=options)
    else:
        raise ValueError(f"Navegador n√£o suportado: {browser}")
    return driver

def acessar_pagina(driver, url):
    logger.info(f"Tentando acessar: {url!r}")
    driver.get(url)
    logger.info(f"Acessou: {url!r}")

def encontrar_elemento(driver, xpath, referencia_map=None, tempo=10):
    try:
        elemento = WebDriverWait(driver, tempo).until(
            EC.presence_of_element_located((By.XPATH, xpath))
        )
        return elemento
    except Exception:
        msg = f"Elemento n√£o encontrado para XPath: {xpath}"
        if referencia_map:
            msg += f" (refer√™ncia map_relative.json: {referencia_map})"
        logger.error(msg)
        send_notification(msg)
        return None

def esperar_elemento(driver, xpath, referencia_map=None, tempo=300):
    elemento = encontrar_elemento(driver, xpath, referencia_map, tempo)
    if not elemento:
        raise Exception(f"Elemento n√£o encontrado: {xpath} (refer√™ncia map_relative.json: {referencia_map})")
    return elemento

def clicar_elemento(driver, xpath, referencia_map=None):
    try:
        elemento = encontrar_elemento(driver, xpath, referencia_map)
        if elemento:
            elemento.click()
    except ElementClickInterceptedException as e:
        logger.warning(f"Click interceptado em {xpath} (refer√™ncia map_relative.json: {referencia_map}). Tentando pressionar ESC para fechar overlay.")
        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)
        time.sleep(1)
        try:
            elemento = encontrar_elemento(driver, xpath, referencia_map)
            if elemento:
                elemento.click()
        except Exception as e2:
            logger.error(f"Falha ao clicar ap√≥s ESC: {e2}")
            raise


def clicar_elemento_real(driver, xpath, referencia_map=None):
    try:
        elemento = encontrar_elemento(driver, xpath, referencia_map, tempo=30)
        if elemento:
            try:
                driver.execute_script("arguments[0].scrollIntoView(true);", elemento)
                ActionChains(driver).move_to_element(elemento).click().perform()
            except Exception:
                # Se falhar com o elemento, tentar encontrar novamente
                elemento = encontrar_elemento(driver, xpath, referencia_map, tempo=5)
                if elemento:
                    driver.execute_script("arguments[0].scrollIntoView(true);", elemento)
                    ActionChains(driver).move_to_element(elemento).click().perform()
    except ElementClickInterceptedException as e:
        logger.warning(f"Click interceptado em {xpath} (refer√™ncia map_relative.json: {referencia_map}). Tentando pressionar ESC para fechar overlay.")
        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)
        time.sleep(1)
        try:
            elemento = encontrar_elemento(driver, xpath, referencia_map, tempo=30)
            if elemento:
                driver.execute_script("arguments[0].scrollIntoView(true);", elemento)
                ActionChains(driver).move_to_element(elemento).click().perform()
        except Exception as e2:
            logger.error(f"Falha ao clicar ap√≥s ESC: {e2}")
            raise

def gerar_otp():
    print("Gerando OTP...")
    response = requests.post(
        otp_url, 
        json={"secret": secret_otp}
    )
    if response.status_code == 200:
        return response.json().get("otp")
    else:
        print(f"Erro ao gerar OTP: {response.status_code} - {response.text}")
        exit(1)

# --- In√≠cio: fun√ß√µes para conex√£o e envio ao SQL Server (Fase 5: SQLAlchemy) ---

def post_records_to_mssql(records, table_name='producao', file_name=None):
    """Envia registros para SQL Server usando SQLAlchemy ORM.
    
    ‚úÖ NOVO (Fase 5): Usa insert_records_sqlalchemy() do package models
    ‚ú® MELHORADO (Fase 14): Logs visuais com progresso, contadores e erros em tempo real
    
    Args:
        records: lista de dicion√°rios com registros a enviar
        table_name: nome da tabela para logs (default: 'producao')
        file_name: nome do arquivo para rastreamento
    
    Features autom√°ticas:
    - Barra de progresso visual (%) durante envio
    - Contadores em tempo real (x/total registros)
    - Emojis informativos para diferentes estados
    - Detalhamento de erros e duplicatas
    - Tratamento de NUL character (0x00)
    - Detec√ß√£o de duplicatas (PRIMARY KEY)
    - Per-record error handling (continua batch mesmo com erros)
    - Type coercion autom√°tico via ORM
    - Logging estruturado em JSONL
    - Taxa de sucesso em porcentagem
    
    Respeita DRY_RUN (se true apenas loga payloads sem enviar).
    
    Returns:
        dict com estat√≠sticas: {success, failed, total, batches_total, batches_failed}
    """
    if not records:
        logger.warning(f"[{table_name}] ‚è≠Ô∏è  Nenhum registro para enviar")
        return {
            'success': 0,
            'failed': 0,
            'total': 0,
            'batches_total': 0,
            'batches_failed': 0
        }
    
    logger.info(f"\n" + "=" * 80)
    logger.info(f"üöÄ INICIANDO ENVIO PARA SQL SERVER")
    logger.info(f"=" * 80)
    logger.info(f"üìä Tabela: {table_name.upper()}")
    logger.info(f"üìÑ Arquivo: {file_name or 'N/A'}")
    logger.info(f"üì¶ Total de registros: {len(records)}")
    logger.info(f"‚úÖ SQLAlchemy ORM ativado (com NUL handling + duplicata detection)")
    logger.info(f"=" * 80)
    
    # Iniciar cron√¥metro
    inicio_envio = time.time()
    
    # Delegar completamente para a fun√ß√£o do ORM
    # Ela trata automaticamente:
    # - NUL characters
    # - Duplicatas
    # - Batching
    # - Retry
    # - Logging JSONL
    stats = insert_records_sqlalchemy(
        records=records,
        table_name=table_name,
        file_name=file_name
    )
    
    # Calcular tempo decorrido
    tempo_decorrido = time.time() - inicio_envio
    
    # Extrair estat√≠sticas
    sucesso = int(stats.get('success', 0))
    erros = int(stats.get('failed', 0))
    duplicatas = int(stats.get('duplicates', 0))
    total = int(stats.get('total', len(records)))
    
    # Calcular percentuais
    taxa_sucesso = (sucesso / total * 100) if total > 0 else 0
    taxa_erro = (erros / total * 100) if total > 0 else 0
    taxa_duplicata = (duplicatas / total * 100) if total > 0 else 0
    
    # Barra de progresso visual
    barra_tamanho = 40
    barra_preenchida = int(barra_tamanho * taxa_sucesso / 100)
    barra = "‚ñà" * barra_preenchida + "‚ñë" * (barra_tamanho - barra_preenchida)
    
    # Determinar emoji de status geral
    if taxa_sucesso >= 95:
        status_emoji = "‚úÖ"
        status_texto = "SUCESSO COMPLETO"
    elif taxa_sucesso >= 70:
        status_emoji = "‚ö†Ô∏è "
        status_texto = "SUCESSO PARCIAL"
    else:
        status_emoji = "‚ùå"
        status_texto = "FALHA"
    
    # Log visual resumido
    logger.info(f"\n" + "=" * 80)
    logger.info(f"{status_emoji} RESULTADO DO ENVIO - {status_texto}")
    logger.info(f"=" * 80)
    logger.info(f"üìà Barra de progresso: [{barra}] {taxa_sucesso:.1f}%")
    logger.info(f"‚úÖ Registros inseridos: {sucesso}/{total} ({taxa_sucesso:.1f}%)")
    
    if duplicatas > 0:
        logger.info(f"‚ö†Ô∏è  Duplicatas detectadas: {duplicatas}/{total} ({taxa_duplicata:.1f}%)")
    
    if erros > 0:
        logger.warning(f"‚ùå Registros com erro: {erros}/{total} ({taxa_erro:.1f}%)")
    
    logger.info(f"‚è±Ô∏è  Tempo decorrido: {tempo_decorrido:.2f}s")
    
    # Calcular velocidade
    if tempo_decorrido > 0:
        velocidade = sucesso / tempo_decorrido
        logger.info(f"üöÑ Velocidade: {velocidade:.0f} registros/segundo")
    
    logger.info(f"=" * 80 + "\n")
    
    # Log detalhado para arquivo (mant√©m registro completo)
    logger.debug(f"[{table_name}] Estat√≠sticas detalhadas: success={sucesso}, failed={erros}, duplicates={duplicatas}, total={total}")
    
    return stats

# --- Fim: fun√ß√µes para conex√£o e envio ao SQL Server (SQLAlchemy) ---

def selecionar_data(driver, xpath, data, referencia_map=None):
    elemento = esperar_elemento(driver, xpath, referencia_map)
    if elemento:
        try:
            elemento.clear()
        except Exception:
            pass  # Alguns campos customizados n√£o suportam clear()
        elemento.click()
        # Simular digita√ß√£o lenta
        for char in data:
            elemento.send_keys(char)
            time.sleep(0.05)
        # Tentar ENTER para fechar o datepicker
        elemento.send_keys(Keys.ENTER)
        time.sleep(0.5)
        valor = elemento.get_attribute('value')
        if valor != data:
            # Tentar clicar no body para fechar o datepicker
            try:
                driver.find_element(By.TAG_NAME, 'body').click()
                time.sleep(0.5)
            except Exception:
                pass
            valor = elemento.get_attribute('value')
            if valor != data:
                logger.warning(f"Campo de data n√£o foi preenchido corretamente: esperado {data}, obtido {valor}")

def selecionar_texto(driver, xpath, text, referencia_map=None):
    # print("Selecionando textos...")
    elemento = esperar_elemento(driver, xpath, referencia_map)
    elemento.click()
    elemento.clear()
    elemento.send_keys(text)
    time.sleep(5)
    driver.find_element(By.XPATH, xpath).send_keys(Keys.ENTER)

def esperar_download_pronto(driver, xpath, referencia_map=None, timeout=60):
    """Espera at√© o link de download estar realmente pronto (href v√°lido)."""
    for _ in range(timeout):
        try:
            elemento = encontrar_elemento(driver, xpath, referencia_map)
            if elemento:
                href = elemento.get_attribute("href")
                if href and href.endswith(".xlsx"):
                    logger.info(f"Link pronto no DOM: {href}")
                    return elemento
        except Exception:
            pass
        time.sleep(1)
    msg = f"Link de download n√£o ficou pronto a tempo: {xpath} (refer√™ncia map_relative.json: {referencia_map})"
    logger.error(msg)
    send_notification(msg)
    raise Exception(msg)

def aguardar_arquivo_disponivel(driver, timeout=300):
    """
    Aguarda o elemento h5 com texto 'Arquivo dispon√≠vel' aparecer no modal.
    Isso indica que o arquivo foi processado e est√° pronto para download.
    
    XPath: /html/body/vaadin-dialog-overlay/vaadin-vertical-layout/h5
    Conte√∫do esperado: "Arquivo dispon√≠vel"
    
    Args:
        driver: Selenium driver
        timeout: Timeout em segundos (default: 300 = 5 minutos)
    
    Returns:
        True se encontrou, lan√ßa exception se timeout
    """
    logger.info(f"‚è≥ Aguardando elemento 'Arquivo dispon√≠vel' (timeout: {timeout}s)...")
    
    xpath_arquivo_disponivel = "/html/body/vaadin-dialog-overlay/vaadin-vertical-layout/h5"
    
    try:
        # Aguardar que o elemento apare√ßa
        WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.XPATH, xpath_arquivo_disponivel))
        )
        
        # Verificar se o texto cont√©m "Arquivo dispon√≠vel"
        elemento = driver.find_element(By.XPATH, xpath_arquivo_disponivel)
        texto = elemento.text.strip()
        
        logger.info(f"‚úÖ Elemento encontrado: '{texto}'")
        
        if "Arquivo dispon√≠vel" in texto or "dispon√≠vel" in texto.lower():
            logger.info("‚úÖ Arquivo est√° pronto para download!")
            return True
        else:
            logger.warning(f"‚ö†Ô∏è Elemento encontrado mas texto inesperado: '{texto}'")
            return True  # Mesmo assim retorna True, pois o elemento existe
            
    except Exception as e:
        logger.error(f"‚ùå Timeout esperando 'Arquivo dispon√≠vel': {e}")
        raise Exception(f"Arquivo n√£o foi processado no tempo esperado ({timeout}s): {e}")

def baixar_arquivo_com_cookies(driver, url, caminho_destino):
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    
    cookies = driver.get_cookies()
    s = requests.Session()
    
    # ‚úÖ NOVO: Configurar retry autom√°tico para SSLError e timeouts
    # Essa configura√ß√£o faz o requests automaticamente tentar novamente em caso de erro SSL
    retry_strategy = Retry(
        total=2,  # Total de retries autom√°ticos (al√©m dos retries manuais)
        status_forcelist=[429, 500, 502, 503, 504],  # Status codes que devem fazer retry
        allowed_methods=["GET"],
        backoff_factor=1  # Esperar 1s, depois 2s, depois 4s entre retries
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    
    for cookie in cookies:
        s.cookies.set(cookie['name'], cookie['value'])
    
    for tentativa in range(1, RETRIES_DOWNLOAD+1):
        try:
            logger.info(f'Tentando baixar arquivo: {url} (tentativa {tentativa}/{RETRIES_DOWNLOAD})')
            
            # ‚úÖ NOVO: Usar timeout mais curto para detectar SSL rapidamente
            # Se der timeout, retry mais r√°pido que 120s
            resposta = s.get(url, stream=True, timeout=30)
            
            if resposta.status_code == 200:
                with open(caminho_destino, 'wb') as f:
                    for chunk in resposta.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                logger.info(f"‚úÖ Arquivo salvo em: {caminho_destino}")
                return True
            else:
                logger.error(f'‚ùå Erro ao baixar arquivo: Status {resposta.status_code}')
                
        except (requests.exceptions.SSLError, requests.exceptions.ConnectionError, 
                requests.exceptions.Timeout) as e:
            # SSL, Connection ou Timeout - pode ser problema transit√≥rio
            logger.error(f'‚ùå Erro de conex√£o ao baixar arquivo: {type(e).__name__}')
            logger.debug(f'   Detalhes: {str(e)[:100]}...')
            
            if tentativa < RETRIES_DOWNLOAD:
                # Para SSL/timeout, usar delay curto (30s) para server recuperar
                delay_adaptativo = 30
                logger.warning(
                    f'‚è≥ Erro transit√≥rio detectado. Aguardando {delay_adaptativo}s '
                    f'antes de tentar novamente... (tentativa {tentativa}/{RETRIES_DOWNLOAD})'
                )
                time.sleep(delay_adaptativo)
            else:
                logger.error(f'‚ùå FALHA FINAL: SSL/Connection error ap√≥s {RETRIES_DOWNLOAD} tentativas')
                
        except Exception as e:
            # Outro erro gen√©rico
            logger.error(f'‚ùå Erro inesperado ao baixar arquivo: {type(e).__name__}: {e}')
            
            if tentativa < RETRIES_DOWNLOAD:
                logger.warning(f'‚è≥ Aguardando {DOWNLOAD_RETRY_DELAY}s antes de tentar novamente...')
                time.sleep(DOWNLOAD_RETRY_DELAY)
            else:
                logger.error(f'‚ùå FALHA FINAL: Erro inesperado ap√≥s {RETRIES_DOWNLOAD} tentativas')
    
    send_notification(f'Falha ao baixar arquivo ap√≥s {RETRIES_DOWNLOAD} tentativas: {url}')
    return False

def inserir_texto(driver, xpath, texto, referencia_map=None):
    elemento = esperar_elemento(driver, xpath, referencia_map)
    if elemento:
        elemento.click()
        elemento.clear()
        elemento.send_keys(texto)

def realizar_download_atividades(driver, button_xpath, tipo_export='atividades'):
    logger.info(f"Realizando download de {tipo_export}...")
    # N√ÉO limpar aqui - limpeza agora acontece UMA VEZ no in√≠cio de executar_rotina()
    
    # Usar tipo_export para logging apropriado
    ref_name = f'atividades.export_{tipo_export}_button'
    clicar_elemento(driver, button_xpath, ref_name)
    
    # Esperar e processar o modal de confirma√ß√£o
    esperar_elemento(driver, XPATHS['atividades']['input_code_field'], 'atividades.input_code_field')
    codigo_elemento = esperar_elemento(driver, XPATHS['atividades']['code_field'], 'atividades.code_field')
    codigo_texto = codigo_elemento.text
    match = re.search(r"(\d+)", codigo_texto)
    if match:
        numero_items = int(match.group(1))
    else:
        raise ValueError(f"N√£o foi poss√≠vel extrair o n√∫mero de {tipo_export} do texto: {codigo_texto}")
    inserir_texto(driver, XPATHS['atividades']['input_code_field'], numero_items, 'atividades.input_code_field')
    clicar_elemento(driver, XPATHS['atividades']['confirm_button'], 'atividades.confirm_button')
    
    # ‚úÖ NOVO: Aguardar que o arquivo seja processado (h5 "Arquivo dispon√≠vel")
    # Timeout: 300 segundos (5 minutos) para processar arquivo
    aguardar_arquivo_disponivel(driver, timeout=300)
    
    # Agora sim, buscar o link de download
    elemento_download = esperar_download_pronto(driver, XPATHS['atividades']['download_link'], 'atividades.download_link', timeout=60)
    url_download = elemento_download.get_attribute('href')
    
    # Nome do arquivo baseado no tipo
    nomes_padrao = {
        'status': 'Exportacao Status.xlsx',
        'atividades': 'Exportacao Atividades.xlsx'
    }
    nome_arquivo = elemento_download.text.strip() or nomes_padrao.get(tipo_export, 'Exportacao.xlsx')
    caminho_destino = os.path.join(DOWNLOADS_DIR, nome_arquivo)
    
    # ‚úÖ Realizar o download com retries
    baixar_arquivo_com_cookies(driver, url_download, caminho_destino)
    clicar_elemento(driver, XPATHS['atividades']['close_button'], 'atividades.close_button')
    fechar_modal(driver)
    logger.info(f"‚úÖ {tipo_export.capitalize()} baixado com sucesso.")

    # NOTA: Parse e envio para SQL Server foi removido desta fun√ß√£o
    # Se precisar enviar, execute: python tests/test_parse_atividades.py <arquivo>
    # Seguido de: python tests/test_post_atividades.py --dry-run

def realizar_download_producao(driver):
    logger.info("Realizando download de produ√ß√£o...")
    # N√ÉO limpar aqui - limpeza agora acontece UMA VEZ no in√≠cio de executar_rotina()
    
    # ‚úÖ NOVO: Aguardar que o arquivo seja processado (h5 "Arquivo dispon√≠vel")
    # Timeout: 300 segundos (5 minutos) para processar arquivo
    aguardar_arquivo_disponivel(driver, timeout=300)
    
    elemento_download = esperar_download_pronto(driver, XPATHS['producao']['download_link'], 'producao.download_link')
    url_download = elemento_download.get_attribute('href')
    nome_arquivo = elemento_download.text.strip() or 'ExportacaoProducao.xlsx'
    caminho_destino = os.path.join(DOWNLOADS_DIR, nome_arquivo)
    baixar_arquivo_com_cookies(driver, url_download, caminho_destino)
    clicar_elemento(driver, XPATHS['producao']['close_button'], 'producao.close_button')
    logger.info("Produ√ß√£o baixado com sucesso.")

    # NOTA: Parse e envio para SQL Server foi removido desta fun√ß√£o
    # Se precisar enviar, execute: python tests/test_parse_atividades.py <arquivo>
    # Seguido de: python tests/test_post_atividades.py --dry-run

def fechar_modal(driver):
    # print("Fechando modal...")
    try:
        overlay = driver.find_element(By.XPATH, XPATHS['common']['modal_overlay'])
        if overlay.is_displayed():
            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)
            WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.XPATH, XPATHS['common']['modal_overlay'])))
            # print("Modal fechado.")
    except Exception:
        print("Nenhum modal aberto.")


def limpar_arquivos_antigos_downloads(palavras=None, padroes_regex=None, extensoes=(".xlsx",)):
    """Remove arquivos antigos na pasta DOWNLOADS_DIR que combinem com palavras-chave ou padr√µes.

    - palavras: lista de palavras-chave para buscar no nome (normalizadas: sem acento, min√∫sculas)
    - padroes_regex: lista de padr√µes regex a testar no nome do arquivo
    - extensoes: tupla de extens√µes alvo (default: .xlsx)

    Retorna o n√∫mero de arquivos removidos.
    """
    import unicodedata

    def normalizar_nome(s):
        s = s or ""
        s = s.lower()
        s = unicodedata.normalize('NFKD', s)
        s = ''.join(ch for ch in s if not unicodedata.combining(ch))
        s = re.sub(r'[^a-z0-9]+', ' ', s)
        s = re.sub(r'\s+', ' ', s).strip()
        return s

    # Defaults de palavras/padr√µes comuns aos relat√≥rios
    if palavras is None:
        palavras = [
            'exportacao', 'exportacao producao', 'producao',
            'exportacao atividades', 'atividades',
            'exportacao status', 'status', 'vivo'
        ]
    if padroes_regex is None:
        padroes_regex = [r'^exportacao.*']

    palavras_norm = [normalizar_nome(p) for p in palavras if p]
    regexes = []
    for p in padroes_regex:
        try:
            regexes.append(re.compile(p, flags=re.IGNORECASE))
        except Exception:
            pass

    downloads = DOWNLOADS_DIR
    removidos = 0
    try:
        for nome in os.listdir(downloads):
            caminho = os.path.join(downloads, nome)
            if not os.path.isfile(caminho):
                continue
            _, ext = os.path.splitext(nome)
            if extensoes and ext.lower() not in [e.lower() for e in extensoes]:
                # Filtra por extens√£o alvo
                continue

            nome_norm = normalizar_nome(nome)
            combina_palavra = any(p in nome_norm for p in palavras_norm)
            combina_regex = any(rx.search(nome) for rx in regexes)

            if combina_palavra or combina_regex:
                try:
                    os.remove(caminho)
                    removidos += 1
                    logger.info(f"Arquivo antigo removido: {nome}")
                except PermissionError as e:
                    logger.warning(f"Nao foi possivel remover (em uso): {nome} - {e}")
                except Exception as e:
                    logger.warning(f"Erro ao remover arquivo antigo: {nome} - {e}")
    except FileNotFoundError:
        os.makedirs(downloads, exist_ok=True)
    except Exception as e:
        logger.warning(f"Erro ao varrer diretorio de downloads: {e}")

    if removidos:
        logger.info(f"Limpeza de downloads concluida: {removidos} arquivo(s) removido(s)")
    else:
        logger.info("Limpeza de downloads: nenhum arquivo alvo encontrado")
    return removidos


# Utilit√°rio: registrar resumo de envio por arquivo/tabela
def registrar_resumo_envio(table_name, file_path, stats, elapsed_seconds):
    """Grava um resumo do envio em logs/envios_resumo.jsonl e loga no console.

    stats esperado: dict com chaves success, failed, total, batches_total, batches_failed.
    """
    try:
        os.makedirs('logs', exist_ok=True)
        resumo_path = os.path.join('logs', 'envios_resumo.jsonl')
        payload = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'table': table_name,
            'file': os.path.basename(file_path) if file_path else None,
            'elapsed_seconds': round(float(elapsed_seconds or 0), 3),
            'success': int(stats.get('success', 0)) if isinstance(stats, dict) else None,
            'failed': int(stats.get('failed', 0)) if isinstance(stats, dict) else None,
            'total': int(stats.get('total', 0)) if isinstance(stats, dict) else None,
            'batches_total': int(stats.get('batches_total', 0)) if isinstance(stats, dict) else None,
            'batches_failed': int(stats.get('batches_failed', 0)) if isinstance(stats, dict) else None,
        }
        with open(resumo_path, 'a', encoding='utf-8') as fo:
            fo.write(json.dumps(payload, ensure_ascii=False) + '\n')
        logger.info(
            f"[Resumo envio] tabela={table_name} arquivo={payload['file']} total={payload['total']} "
            f"sucesso={payload['success']} falha={payload['failed']} tempo={payload['elapsed_seconds']}s"
        )
    except Exception as e:
        logger.warning(f"Falha ao registrar resumo de envio: {e}")


# --- In√≠cio: fun√ß√µes para processamento do ExportacaoProducao.xlsx e envio ---
def parse_export_producao(file_path):
    """Parse flex√≠vel usando pandas e sql_map.json para mapeamento de colunas.

    Melhorias:
    - Usa APENAS sql_map.json (n√£o precisa de nocodb_map.json)
    - Normaliza cabe√ßalhos (remove acentos, pontua√ß√£o, lower) e faz matching tolerante
    - Tenta converter strings que parecem datas para formato `%Y-%m-%d %H:%M:%S`
    - Mant√©m concatena√ß√£o de colunas extras em `TAGS`
    """
    import pandas as pd
    import unicodedata

    # Procura sql_map.json sempre em \bases\
    sql_map_file = os.path.join(os.path.dirname(__file__), 'bases', 'sql_map.json')
    if not os.path.exists(sql_map_file):
        raise FileNotFoundError(f"Arquivo sql_map.json n√£o encontrado em: {sql_map_file}")

    with open(sql_map_file, 'r', encoding='utf-8') as f:
        sql_map = json.load(f)

    base_name = os.path.basename(file_path)
    
    # Buscar entrada no sql_map para este arquivo
    map_entry = sql_map.get(base_name)
    if not map_entry:
        logger.warning(f"Nenhum mapeamento encontrado em {sql_map_file} para {base_name}")
        return []
    
    # expected_headers agora vem do sql_map (colunas do banco)
    expected_headers = map_entry.get('colunas', [])
    if not expected_headers:
        logger.warning(f"Nenhuma coluna mapeada para {base_name}")
        return []

    def normalize_header(s):
        if s is None:
            return ''
        s = str(s).strip().lower()
        # Remove acentos
        s = unicodedata.normalize('NFKD', s)
        s = ''.join(ch for ch in s if not unicodedata.combining(ch))
        # Substitui caracteres n√£o alfanum√©ricos por espa√ßo
        s = re.sub(r'[^a-z0-9]+', ' ', s)
        s = re.sub(r'\s+', ' ', s).strip()
        return s

    # Ler a planilha com pandas
    try:
        df = pd.read_excel(file_path, engine='openpyxl')
    except Exception as e:
        logger.error(f"Erro ao ler Excel {file_path}: {e}")
        return []

    # Preparar mapeamento tolerante entre cabe√ßalhos esperados (do banco) e colunas reais (do Excel)
    df.columns = [str(c).strip() if pd.notna(c) else '' for c in df.columns]
    real_columns = list(df.columns)

    expected_norm_map = {normalize_header(h): h for h in expected_headers}
    col_to_expected = {}

    for col in real_columns:
        norm = normalize_header(col)
        if norm in expected_norm_map:
            col_to_expected[col] = expected_norm_map[norm]
        else:
            # tentar match parcial (contain)
            found = None
            for en_norm, orig in expected_norm_map.items():
                if en_norm and (en_norm in norm or norm in en_norm):
                    found = orig
                    break
            if found:
                col_to_expected[col] = found
            else:
                col_to_expected[col] = None  # ser√° tratado como TAG extra

    # Fun√ß√£o utilit√°ria para formatar valores
    def format_value(val):
        if pd.isna(val):
            return ""
        if isinstance(val, datetime):
            return val.strftime("%Y-%m-%d %H:%M:%S")
        # tentar converter strings que parecem datas
        if isinstance(val, str):
            val_str = val.strip()
            
            # üÜï Remover caracteres NUL (0x00) que causam erro no SQL
            val_str = val_str.replace('\x00', '')
            
            if not val_str:
                return ""
            
            # üÜï Ignora FutureWarning do pandas para timezones desconhecidos (ex: "10:30 AS 12:30")
            import warnings
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", FutureWarning)
                
                try:
                    # Se cont√©m " AS " (range de hor√°rios), retornar como string (n√£o tentar parse)
                    if " AS " in val_str.upper():
                        return val_str
                    
                    parsed = pd.to_datetime(val_str, dayfirst=True, errors='coerce')
                    if pd.notna(parsed):
                        return parsed.to_pydatetime().strftime("%Y-%m-%d %H:%M:%S")
                except Exception:
                    pass
            return val_str
        if isinstance(val, (int, float)):
            return str(val).replace('.0', '')
        return str(val)

    records = []
    for line_num, (_, row) in enumerate(df.iterrows(), start=2):  # start=2 porque linha 1 √© header
        rec = {}
        tags_extra = []
        for col, val in row.items():
            mapped = col_to_expected.get(col)
            if mapped:
                rec[mapped] = format_value(val)
            else:
                # coluna sem mapeamento -> adicionar √†s tags extras
                if not pd.isna(val):
                    tags_extra.append(str(val).strip())

        # Garantir todas as chaves esperadas existam com string vazia
        for h in expected_headers:
            if h not in rec or rec[h] is None:
                rec[h] = ""

        # Concatena tags no campo 'TAGS'
        if 'TAGS' in rec:
            existing = rec.get('TAGS', "")
            parts = []
            if existing:
                parts.append(existing)
            parts.extend([t for t in tags_extra if t])
            rec['TAGS'] = ' | '.join(parts) if parts else ""
        else:
            rec['TAGS'] = ' | '.join([t for t in tags_extra if t]) if tags_extra else ""

        # Adicionar n√∫mero da linha (interno, n√£o ser√° enviado ao banco)
        rec['_line_number'] = line_num
        rec['_file_name'] = os.path.basename(file_path)

        records.append(rec)

    logger.info(f"Parsed {len(records)} registros de {file_path} (pandas, flex) usando mapeamento {base_name}")
    return records

def parse_export_atividades(file_path):
    """Parse de arquivo de Atividades - usa mesma l√≥gica flex√≠vel de parse_export_producao."""
    return parse_export_producao(file_path)

def parse_export_status(file_path):
    """Parse de arquivo de Status com mapeamento correto de colunas duplicadas.
    
    ‚úÖ Problema Resolvido (Phase 15.1):
    - Excel: USU√ÅRIO, USU√ÅRIO.1 (duas colunas com mesmo nome)
    - Banco: USUARIO, USUARIO_1 (renomeadas para diferenciar)
    - Este function mapeia automaticamente os nomes!
    """
    import pandas as pd
    
    # Usar parse flex√≠vel
    records = parse_export_producao(file_path)
    
    # Mapear colunas duplicadas de Excel para nomes do banco
    # Excel: USU√ÅRIO ‚Üí Banco: USUARIO
    # Excel: USU√ÅRIO.1 ‚Üí Banco: USUARIO_1
    for record in records:
        keys_to_rename = []
        
        for key in list(record.keys()):
            # Procurar por USU√ÅRIO (com ou sem acento)
            if 'USU√ÅRIO' in key or 'usuario' in key.lower():
                if key == 'USU√ÅRIO':
                    # Primeira coluna: renomear para USUARIO (mapeamento esperado pelo banco)
                    keys_to_rename.append((key, 'USUARIO'))
                elif key == 'USU√ÅRIO.1':
                    # Segunda coluna: renomear para USUARIO_1 (mapeamento esperado pelo banco)
                    keys_to_rename.append((key, 'USUARIO_1'))
        
        # Aplicar renomea√ß√µes
        for old_key, new_key in keys_to_rename:
            if old_key in record:
                record[new_key] = record.pop(old_key)
    
    return records

def parse_only(file_path):
    """
    üÜï Parse PURO - sem logging, sem banco, sem side effects
    
    Use esta fun√ß√£o em testes que precisam APENAS de parse para JSON
    N√£o dispara nenhuma opera√ß√£o de banco de dados
    
    Args:
        file_path: Caminho para arquivo Excel
    
    Returns:
        Lista de dicts com registros parseados
    """
    # Apenas retorna o resultado do parse sem nenhum logging que poderia disparar side effects
    return parse_export_producao(file_path)

def post_records_to_nocodb(records, table_url=None, table_name='producao'):
    """Envia registros em batches com retry/backoff.
    
    Args:
        records: lista de dicion√°rios com registros a enviar
        table_url: URL da tabela (default: l√™ URL_TABELA_PRODUCAO do .env)
        table_name: nome da tabela para logs (default: 'producao')
    
    Respeita DRY_RUN (se true apenas loga payloads).
    """
    url = table_url or os.getenv('URL_TABELA_PRODUCAO')
    token = os.getenv('BEARER_TOKEN')
    if not url or not token:
        raise ValueError('URL da tabela e/ou BEARER_TOKEN n√£o configurados no .env')

    headers = {
        'Authorization': f'Bearer {token}',
        'Content-Type': 'application/json'
    }

    os.makedirs('logs', exist_ok=True)
    out_file = os.path.join('logs', f'sent_records_{table_name}.jsonl')

    success = 0
    failed = 0
    batches_total = 0
    batches_failed = 0

    def chunks(lst, n):
        """Divide lista em chunks de tamanho n."""
        for i in range(0, len(lst), n):
            yield lst[i:i + n]

    # Processar todos os registros primeiro
    processed_records = []
    for record in records:
        processed_record = {}
        for key, value in record.items():
            if value is None or str(value).lower() in ('none', 'nan', ''):
                processed_record[key] = ""
            elif isinstance(value, datetime):
                processed_record[key] = value.strftime("%Y-%m-%d %H:%M:%S")
            elif isinstance(value, (int, float)):
                processed_record[key] = str(value).replace(".0", "")
            else:
                processed_record[key] = str(value).strip()
        processed_records.append(processed_record)

    # Dividir em batches
    batches = list(chunks(processed_records, BATCH_SIZE))
    total_batches = len(batches)
    logger.info(f"[{table_name}] Iniciando envio de {len(processed_records)} registros em {total_batches} batches de at√© {BATCH_SIZE} registros cada")

    # Processar cada batch
    for batch_num, batch in enumerate(batches, 1):
        batches_total += 1
        logger.info(f"[{table_name}] Processando batch {batch_num}/{total_batches} com {len(batch)} registros")

        if DRY_RUN:
            logger.info(f"[{table_name}] DRY_RUN ativo. Payload batch {batch_num}: {json.dumps(batch[:1], ensure_ascii=False)} (...)")
            with open(out_file, 'a', encoding='utf-8') as fo:
                for record in batch:
                    fo.write(json.dumps({
                        'status': 'DRY_RUN',
                        'payload': record
                    }, default=str, ensure_ascii=False) + '\n')
            success += len(batch)
            continue

        # Tentar enviar o batch
        attempt = 0
        batch_sent = False
        while attempt < POST_RETRIES and not batch_sent:
            try:
                logger.info(f"[{table_name}] Tentativa {attempt + 1} de envio do batch {batch_num}")
                resp = requests.post(url, headers=headers, json=batch, timeout=60)
                
                if resp.status_code in (200, 201):
                    success += len(batch)
                    batch_sent = True
                    try:
                        resjson = resp.json()
                    except Exception:
                        resjson = {'raw_text': resp.text}
                    with open(out_file, 'a', encoding='utf-8') as fo:
                        for record in batch:
                            fo.write(json.dumps({
                                'status': 'sent',
                                'record': record,
                                'response': resjson
                            }, default=str, ensure_ascii=False) + '\n')
                    break
                else:
                    attempt += 1
                    logger.warning(
                        f"[{table_name}] Falha no envio do batch {batch_num}. Status={resp.status_code} "
                        f"Response={resp.text} Tentativa={attempt}/{POST_RETRIES}"
                    )
                    if attempt >= POST_RETRIES:
                        failed += len(batch)
                        batches_failed += 1
                        with open(out_file, 'a', encoding='utf-8') as fo:
                            for record in batch:
                                fo.write(json.dumps({
                                    'status': 'failed',
                                    'record': record,
                                    'response_status': resp.status_code,
                                    'response_text': resp.text
                                }, default=str, ensure_ascii=False) + '\n')
                    else:
                        time.sleep(BACKOFF_BASE ** attempt)
            except Exception as e:
                attempt += 1
                logger.error(f"[{table_name}] Erro ao enviar batch {batch_num}: {e} tentativa={attempt}/{POST_RETRIES}")
                if attempt >= POST_RETRIES:
                    failed += len(batch)
                    batches_failed += 1
                    with open(out_file, 'a', encoding='utf-8') as fo:
                        for record in batch:
                            fo.write(json.dumps({
                                'status': 'failed',
                                'record': record,
                                'error': str(e)
                            }, default=str, ensure_ascii=False) + '\n')
                    break
                time.sleep(BACKOFF_BASE ** attempt)

        if batch_num < total_batches:
            # Pequena pausa entre batches para n√£o sobrecarregar a API
            time.sleep(0.5)

    total = success + failed
    logger.info(
        f"[{table_name}] Envio conclu√≠do: {success} sucesso(s), {failed} falha(s) de {total} total "
        f"({batches_failed} de {batches_total} batches falharam)"
    )
    return {
        'success': success,
        'failed': failed,
        'total': total,
        'batches_total': batches_total,
        'batches_failed': batches_failed
    }

# --- Fim: fun√ß√µes para processamento do ExportacaoProducao.xlsx e envio ---

def exportAtividadesStatus(driver):
    """Exporta relat√≥rio de Status de Atividades com retry autom√°tico (3 tentativas, com delay configur√°vel)."""
    logger.info("Exportando atividades<>status...")
    
    max_tentativas = 3
    delay_segundos = DOWNLOAD_RETRY_DELAY  # ‚úÖ Usa delay configur√°vel do .env (default: 120s)
    
    for tentativa in range(1, max_tentativas + 1):
        try:
            esperar_elemento(driver, XPATHS['atividades']['panel'], 'atividades.panel')
            clicar_elemento(driver, XPATHS['atividades']['panel'], 'atividades.panel')
            
            data_atual = datetime.now()
            data_90_dias_atras = data_atual - timedelta(days=90)
            data_inicial = data_90_dias_atras.strftime("%d/%m/%Y")
            
            selecionar_data(driver, XPATHS['atividades']['date_picker'], data_inicial, 'atividades.date_picker')
            clicar_elemento(driver, XPATHS['atividades']['search_button'], 'atividades.search_button')
            realizar_download_atividades(driver, XPATHS['atividades']['export_status_button'], 'status')
            fechar_modal(driver)
            
            logger.info("‚úÖ Status de Atividades baixado com sucesso!")
            return  # Sucesso, sair da fun√ß√£o
            
        except Exception as e:
            if tentativa < max_tentativas:
                logger.warning(
                    f"‚ö†Ô∏è Erro ao baixar Status (tentativa {tentativa}/{max_tentativas}): {type(e).__name__}\n"
                    f"   Aguardando {delay_segundos}s antes de tentar novamente..."
                )
                time.sleep(delay_segundos)
            else:
                logger.error(
                    f"‚ùå FALHA FINAL: N√£o consegui baixar Status ap√≥s {max_tentativas} tentativas.\n"
                    f"   Erro: {e}"
                )
                raise

def exportAtividades(driver):
    """Exporta relat√≥rio de Atividades com retry autom√°tico (3 tentativas, 1 min delay)."""
    logger.info("Exportando atividades...")
    
    max_tentativas = 3
    delay_segundos = DOWNLOAD_RETRY_DELAY  # ‚úÖ Usa delay configur√°vel do .env (default: 120s)
    
    for tentativa in range(1, max_tentativas + 1):
        try:
            # Garantir que o painel est√° aberto (reabrir se necess√°rio)
            esperar_elemento(driver, XPATHS['atividades']['panel'], 'atividades.panel')
            clicar_elemento(driver, XPATHS['atividades']['panel'], 'atividades.panel')
            time.sleep(1)  # Pequeno delay para painel abrir
            
            # Aguardar que o grid de dados apare√ßa
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, XPATHS['atividades']['search_button']))
            )
            
            data_atual = datetime.now()
            data_90_dias_atras = data_atual - timedelta(days=90)
            data_inicial = data_90_dias_atras.strftime("%d/%m/%Y")
            
            selecionar_data(driver, XPATHS['atividades']['date_picker'], data_inicial, 'atividades.date_picker')
            clicar_elemento(driver, XPATHS['atividades']['search_button'], 'atividades.search_button')
            time.sleep(2)  # Aguardar grid carregar com dados
            
            # Aguardar que o bot√£o de export esteja vis√≠vel
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, XPATHS['atividades']['export_atividades_button']))
            )
            
            realizar_download_atividades(driver, XPATHS['atividades']['export_atividades_button'], 'atividades')
            fechar_modal(driver)
            
            logger.info("‚úÖ Atividades baixado com sucesso!")
            return  # Sucesso, sair da fun√ß√£o
            
        except Exception as e:
            if tentativa < max_tentativas:
                logger.warning(
                    f"‚ö†Ô∏è Erro ao baixar Atividades (tentativa {tentativa}/{max_tentativas}): {type(e).__name__}\n"
                    f"   Aguardando {delay_segundos}s antes de tentar novamente..."
                )
                time.sleep(delay_segundos)
            else:
                logger.error(
                    f"‚ùå FALHA FINAL: N√£o consegui baixar Atividades ap√≥s {max_tentativas} tentativas.\n"
                    f"   Erro: {e}"
                )
                raise

def exportProducao(driver):
    """Exporta relat√≥rio de Produ√ß√£o com retry autom√°tico (3 tentativas, 1 min delay)."""
    logger.info("Exportando produ√ß√£o...")
    
    max_tentativas = 3
    delay_segundos = DOWNLOAD_RETRY_DELAY  # ‚úÖ Usa delay configur√°vel do .env (default: 120s)
    
    for tentativa in range(1, max_tentativas + 1):
        try:
            esperar_elemento(driver, XPATHS['producao']['panel'], 'producao.panel')
            clicar_elemento(driver, XPATHS['producao']['panel'], 'producao.panel')

            data_atual = datetime.now()
            data_90_dias_atras = data_atual - timedelta(days=92)
            data_inicial_ajustada = data_90_dias_atras.replace(day=1)
            data_inicial = data_inicial_ajustada.strftime("%d/%m/%Y")
            texto = "Painel de Produ√ß√£o Vivo"

            selecionar_data(driver, XPATHS['producao']['date_picker'], data_inicial, 'producao.date_picker')
            selecionar_texto(driver, XPATHS['producao']['combo_box'], texto, 'producao.combo_box')
            clicar_elemento(driver, XPATHS['producao']['radio_button'], 'producao.radio_button')
            clicar_elemento(driver, XPATHS['producao']['search_button'], 'producao.search_button')
            realizar_download_producao(driver)
            fechar_modal(driver)
            
            logger.info("‚úÖ Produ√ß√£o baixado com sucesso!")
            return  # Sucesso, sair da fun√ß√£o
            
        except Exception as e:
            if tentativa < max_tentativas:
                logger.warning(
                    f"‚ö†Ô∏è Erro ao baixar Produ√ß√£o (tentativa {tentativa}/{max_tentativas}): {type(e).__name__}\n"
                    f"   Aguardando {delay_segundos}s antes de tentar novamente..."
                )
                time.sleep(delay_segundos)
            else:
                logger.error(
                    f"‚ùå FALHA FINAL: N√£o consegui baixar Produ√ß√£o ap√≥s {max_tentativas} tentativas.\n"
                    f"   Erro: {e}"
                )
                raise

def login(driver):
    acessar_pagina(driver, url)
    logger.info("Realizando login...")
    esperar_elemento(driver, XPATHS['login']['username_field'], 'login.username_field')
    inserir_texto(driver, XPATHS['login']['username_field'], username, 'login.username_field')
    inserir_texto(driver, XPATHS['login']['password_field'], password, 'login.password_field')
    while True:
        otp = gerar_otp()
        clicar_elemento(driver, XPATHS['login']['otp_radio'], 'login.otp_radio')
        clicar_elemento(driver, XPATHS['login']['otp_field'], 'login.otp_field')
        inserir_texto(driver, XPATHS['login']['otp_field'], otp, 'login.otp_field')
        clicar_elemento(driver, XPATHS['login']['login_button'], 'login.login_button')
        time.sleep(2)
        try:
            mensagem = driver.find_element(By.XPATH, XPATHS['login']['error_message']).text
            if mensagem in ["Usu√°rio n√£o encontrado", "C√≥digo autenticador inv√°lido", "Usu√°rio inexistente ou senha inv√°lida"]:
                logger.warning("Erro detectado, tentando novamente...")
                continue
        except:
            break
    logger.info("Login realizado com sucesso!")

def abrir_sidebar(driver):
    logger.info("Abrindo sidebar...")
    try:
        sidebar = esperar_elemento(driver, XPATHS['atividades']['sidebar'], 'atividades.sidebar')
        if sidebar:
            # Adiciona o atributo drawer-opened ao elemento sidebar
            driver.execute_script("arguments[0].setAttribute('drawer-opened', '');", sidebar)
            logger.info("Atributo drawer-opened adicionado ao sidebar com sucesso!")
            time.sleep(1)  # Pequena pausa para garantir que a UI responda
    except Exception as e:
        logger.warning(f"Erro ao abrir sidebar: {e}")
        # Continua mesmo se houver erro, pois isso n√£o deve interromper o fluxo principal

def logout(driver):
    logger.info("Realizando logout...")
    esperar_elemento(driver, XPATHS['logout']['logout_button'], 'logout.logout_button')
    clicar_elemento(driver, XPATHS['logout']['logout_button'], 'logout.logout_button')
    esperar_elemento(driver, XPATHS['logout']['logout_option'], 'logout.logout_option')
    clicar_elemento(driver, XPATHS['logout']['logout_option'], 'logout.logout_option')

def mover_arquivos(diretorio_origem, arquivos, diretorio_destino, subdiretorio):
    logger.info("Iniciando movimenta√ß√£o segura de arquivos...")    
    os.makedirs(diretorio_destino, exist_ok=True) # Garantir estrutura de diret√≥rios
    historico_path = os.path.join(diretorio_destino, subdiretorio)
    os.makedirs(historico_path, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    arquivos_movidos = 0
    arquivos_ignorados = 0

    for arquivo in arquivos:
        dest_file = os.path.join(diretorio_destino, arquivo)
        orig_file = os.path.join(diretorio_origem, arquivo)        

        if os.path.exists(orig_file): # Verifica se o arquivo de origem existe
            try:
                if os.path.exists(dest_file): # Gerenciar conflitos no destino
                    logger.warning(f"üí• Conflito detectado: {arquivo}")
                    try:
                        nome, ext = os.path.splitext(arquivo) # Gerar novo nome √∫nico
                        nome_salvo = f"{nome}_BACKUP_{timestamp}{ext}"
                        shutil.move(dest_file, os.path.join(historico_path, nome_salvo)) # Mover arquivo conflitante para hist√≥rico
                        logger.info(f"‚úÖ Backup criado: {nome_salvo}")
                    except PermissionError as e:
                        logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel criar backup de {arquivo}: {e}")
                        logger.warning(f"‚ö†Ô∏è Ignorando movimenta√ß√£o do arquivo: {arquivo}")
                        arquivos_ignorados += 1
                        continue
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erro ao criar backup de {arquivo}: {e}")
                        logger.warning(f"‚ö†Ô∏è Ignorando movimenta√ß√£o do arquivo: {arquivo}")
                        arquivos_ignorados += 1
                        continue

                shutil.move(orig_file, dest_file) # Mover arquivo original para o destino
                logger.info(f"‚û°Ô∏è {arquivo} movido para destino")
                arquivos_movidos += 1
            except PermissionError as e:
                logger.warning(f"‚ö†Ô∏è Arquivo em uso por outro processo: {arquivo}")
                logger.warning(f"‚ö†Ô∏è Detalhes do erro: {e}")
                logger.warning(f"‚ö†Ô∏è Ignorando movimenta√ß√£o do arquivo e continuando o fluxo...")
                arquivos_ignorados += 1
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao mover arquivo {arquivo}: {e}")
                logger.warning(f"‚ö†Ô∏è Ignorando movimenta√ß√£o do arquivo e continuando o fluxo...")
                arquivos_ignorados += 1
        else:
            logger.warning(f"‚ö†Ô∏è Arquivo ausente: {orig_file}")
            arquivos_ignorados += 1
    
    logger.info(f"Opera√ß√£o conclu√≠da: {arquivos_movidos} arquivo(s) movido(s), {arquivos_ignorados} arquivo(s) ignorado(s)\n")

def processar_arquivos_baixados():
    """Processa arquivos baixados: parse e envio ao banco de dados.
    
    Executa AP√ìS todos os downloads estarem completos.
    ‚ú® MELHORADO (Fase 14): Logs visuais com progresso e indicadores de estado
    """
    logger.info("\n" + "=" * 80)
    logger.info("üì§ FASE 2: Processando e enviando arquivos para o banco")
    logger.info("=" * 80)
    
    if not os.path.exists(DOWNLOADS_DIR):
        logger.warning(f"‚è≠Ô∏è  Diret√≥rio de downloads n√£o existe: {DOWNLOADS_DIR}")
        return None
    
    # Mapear arquivos Excel para suas configura√ß√µes de processamento
    arquivos_para_processar = [
        {
            'nome': 'Exportacao Status.xlsx',
            'tabela': 'status',
            'descricao': 'Status de Atividades'
        },
        {
            'nome': 'Exportacao Atividade.xlsx',
            'tabela': 'atividade',
            'descricao': 'Atividades'
        },
        {
            'nome': 'ExportacaoProducao.xlsx',
            'tabela': 'producao',
            'descricao': 'Produ√ß√£o'
        }
    ]
    
    processados_sucesso = 0
    processados_erro = 0
    total_registros_enviados = 0
    total_registros_falhados = 0
    
    for idx, arquivo_info in enumerate(arquivos_para_processar, 1):
        nome_arquivo = arquivo_info['nome']
        tabela_nome = arquivo_info['tabela']
        descricao = arquivo_info['descricao']
        
        caminho_arquivo = os.path.join(DOWNLOADS_DIR, nome_arquivo)
        
        # Verificar se arquivo existe
        if not os.path.exists(caminho_arquivo):
            logger.warning(f"‚è≠Ô∏è  [{idx}/3] {descricao}: Arquivo n√£o encontrado (pulando)")
            continue
        
        try:
            logger.info(f"\n" + "-" * 80)
            logger.info(f"üìã [{idx}/3] Processando: {descricao}")
            logger.info("-" * 80)
            
            # Informa√ß√µes do arquivo
            tamanho_kb = os.path.getsize(caminho_arquivo) / 1024
            logger.info(f"üìÅ Arquivo: {nome_arquivo}")
            logger.info(f"üì¶ Tamanho: {tamanho_kb:.1f} KB")
            
            # Parse do arquivo
            logger.info(f"üîç Fazendo parse do arquivo...")
            records = parse_export_producao(caminho_arquivo)
            
            if not records:
                logger.warning(f"‚ö†Ô∏è  Nenhum registro encontrado no arquivo (pulando)")
                processados_erro += 1
                continue
            
            logger.info(f"‚úÖ Parse conclu√≠do: {len(records)} registros extra√≠dos")
            
            # Envio ao banco de dados com indicador visual
            logger.info(f"")
            logger.info(f"üì§ Enviando para SQL Server ({len(records)} registros)...")
            logger.info(f"‚è±Ô∏è  Aguarde enquanto os dados s√£o processados...")
            
            inicio = time.time()
            stats = post_records_to_mssql(records, table_name=tabela_nome, file_name=nome_arquivo)
            duracao = time.time() - inicio
            
            # Registrar resumo
            registrar_resumo_envio(tabela_nome, caminho_arquivo, stats, duracao)
            
            # Atualizar contadores globais
            total_registros_enviados += stats.get('success', 0)
            total_registros_falhados += stats.get('failed', 0)
            
            logger.info(f"‚úÖ [{idx}/3] {descricao}: Processamento conclu√≠do com sucesso!")
            processados_sucesso += 1
            
        except Exception as e:
            logger.error(f"‚ùå [{idx}/3] {descricao}: Erro ao processar")
            logger.error(f"   Tipo de erro: {type(e).__name__}")
            logger.error(f"   Mensagem: {str(e)[:200]}")
            logger.debug(f"   Detalhes completos:", exc_info=True)
            processados_erro += 1
    
    # Resumo final
    logger.info(f"\n" + "=" * 80)
    logger.info(f"üìä RESUMO CONSOLIDADO DE PROCESSAMENTO")
    logger.info("=" * 80)
    logger.info(f"üìÅ Arquivos processados: {processados_sucesso}/3")
    logger.info(f"   ‚úÖ Sucesso: {processados_sucesso}")
    logger.info(f"   ‚ùå Erros: {processados_erro}")
    logger.info(f"")
    logger.info(f"üì¶ Registros totais:")
    logger.info(f"   ‚úÖ Enviados com sucesso: {total_registros_enviados}")
    logger.info(f"   ‚ùå Falhados: {total_registros_falhados}")
    logger.info(f"   üìä Taxa geral: {(total_registros_enviados/(total_registros_enviados + total_registros_falhados)*100 if (total_registros_enviados + total_registros_falhados) > 0 else 0):.1f}%")
    logger.info("=" * 80 + "\n")
    
    return {
        'sucesso': processados_sucesso,
        'erro': processados_erro,
        'total': processados_sucesso + processados_erro,
        'registros_sucesso': total_registros_enviados,
        'registros_erro': total_registros_falhados
    }

def limpar_logs():
    """
    üîß NOVO (Fase 10): Limpa pasta logs antes de cada execu√ß√£o
    Remove TODOS os arquivos de log (robo_download.log, error_records_*.jsonl, sent_records_*.jsonl, etc)
    """
    logs_dir = Path('logs')
    
    if not logs_dir.exists():
        return 0
    
    removidos = 0
    
    # Remove TODOS os arquivos na pasta logs
    for arquivo in logs_dir.glob('*'):
        if arquivo.is_file():
            try:
                arquivo.unlink()
                logger.info(f"üóëÔ∏è  Log removido: {arquivo.name}")
                removidos += 1
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  N√£o foi poss√≠vel remover log: {arquivo.name} - {e}")
    
    if removidos > 0:
        logger.info(f"‚úÖ Limpeza de logs: {removidos} arquivo(s) removido(s)")
    
    return removidos

def executar_rotina():
    etapas = []
    try:
        data_atual = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        logger.info(f"Iniciando execu√ß√£o em {data_atual}")
        etapas.append(f"Execu√ß√£o iniciada em {data_atual}")
        
        # üîß NOVO (Fase 10): Limpar logs da execu√ß√£o anterior
        logger.info("\n" + "=" * 70)
        logger.info("üßπ LIMPEZA PR√â-EXECU√á√ÉO")
        logger.info("=" * 70)
        limpar_logs()
        
        # Limpa a pasta de screenshots
        screenshots_dir = Path('element_screenshots')
        if screenshots_dir.exists():
            for arquivo in screenshots_dir.glob('*'):
                try:
                    arquivo.unlink()
                    logger.info(f"Screenshot removido: {arquivo.name}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel remover screenshot: {arquivo.name} - {e}")
        
        # Limpeza robusta da pasta Downloads antes de qualquer novo download
        removidos = limpar_arquivos_antigos_downloads()
        logger.info(f"Limpeza inicial de Downloads: {removidos} arquivo(s) removidos")
        
        # ========== FASE 1: DOWNLOADS ==========
        logger.info("\n" + "=" * 70)
        logger.info("üì• FASE 1: Baixando todos os arquivos...")
        logger.info("=" * 70)
        
        etapas.append("Driver iniciado")
        driver = iniciar_driver()
        etapas.append("Login realizado")
        login(driver)
        
        # Abrir sidebar
        abrir_sidebar(driver)
        etapas.append("Sidebar aberto")
        
        # Exporta√ß√£o Atividades Status
        data_atual_dt = datetime.now()
        data_90_dias_atras = data_atual_dt - timedelta(days=90)
        data_inicial_atividades = data_90_dias_atras.strftime("%d/%m/%Y")
        etapas.append(f"Exporta√ß√£o Atividades Status (data inicial: {data_inicial_atividades})")
        logger.info("\n[1/3] Baixando Status de Atividades...")
        exportAtividadesStatus(driver)
        time.sleep(10)
        logger.info("‚úÖ Status de Atividades baixado!")
        
        # Abrir sidebar
        abrir_sidebar(driver)
        
        # Exporta√ß√£o Atividades
        etapas.append(f"Exporta√ß√£o Atividades (data inicial: {data_inicial_atividades})")
        logger.info("\n[2/3] Baixando Atividades...")
        exportAtividades(driver)
        time.sleep(10)
        logger.info("‚úÖ Atividades baixado!")
        
        # Abrir sidebar
        abrir_sidebar(driver)
        
        # Exporta√ß√£o Produ√ß√£o
        data_90_dias_atras_producao = data_atual_dt - timedelta(days=92)
        data_inicial_ajustada = data_90_dias_atras_producao.replace(day=1)
        data_inicial_producao = data_inicial_ajustada.strftime("%d/%m/%Y")
        etapas.append(f"Exporta√ß√£o Produ√ß√£o (data inicial: {data_inicial_producao})")
        logger.info("\n[3/3] Baixando Produ√ß√£o...")
        exportProducao(driver)
        logger.info("‚úÖ Produ√ß√£o baixado!")
        
        # Fechar driver ap√≥s todos os downloads
        driver.quit()
        etapas.append("Driver finalizado")
        time.sleep(5)
        
        logger.info("\n" + "=" * 70)
        logger.info("‚úÖ FASE 1 CONCLU√çDA: Todos os arquivos foram baixados!")
        logger.info("=" * 70)
        
        logger.info(f"Arquivos em: {DOWNLOADS_DIR}")
        if os.path.exists(DOWNLOADS_DIR):
            arquivos = [f for f in os.listdir(DOWNLOADS_DIR) if f.endswith('.xlsx')]
            logger.info(f"Total de arquivos: {len(arquivos)}")
            for arq in sorted(arquivos):
                tamanho = os.path.getsize(os.path.join(DOWNLOADS_DIR, arq)) / 1024
                logger.info(f"   üìÑ {arq} ({tamanho:.1f} KB)")
        
        # ========== FASE 2: PROCESSAMENTO E ENVIO ==========
        etapas.append("Iniciando processamento de arquivos")
        resultado_processamento = processar_arquivos_baixados()
        
        # Verificar se resultado n√£o √© None antes de acessar
        if resultado_processamento and resultado_processamento.get('sucesso', 0) > 0:
            logger.info(f"\n‚úÖ {resultado_processamento['sucesso']} arquivo(s) processado(s) com sucesso!")
            logger.info(f"   Registros enviados: {resultado_processamento.get('registros_sucesso', 0)}")
        if resultado_processamento and resultado_processamento.get('erro', 0) > 0:
            logger.warning(f"‚ö†Ô∏è {resultado_processamento['erro']} arquivo(s) com erro durante processamento")
            logger.warning(f"   Registros falhados: {resultado_processamento.get('registros_erro', 0)}")
        
        etapas.append("Arquivos processados e enviados ao banco")
        data_atual = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        logger.info(f"\nFinalizado em {data_atual}")
        etapas.append(f"Finalizado em {data_atual}")
        logger.info("Agendando a execu√ß√£o a cada 30 minutos...")
        
    except Exception as e:
        etapas.append(f"Erro: {e}")
        logger.error(f"Erro: {e}")
        # N√£o tratar erros de arquivo em uso como cr√≠ticos
        if "WinError 32" in str(e) and "j√° est√° sendo usado por outro processo" in str(e):
            logger.warning(f"Arquivo em uso detectado: {e}")
            logger.warning("Este erro n√£o √© cr√≠tico e o fluxo continuar√° normalmente.")
        else:
            send_notification(f"Erro cr√≠tico na execu√ß√£o: {e}")
    finally:
        print("\nResumo das etapas executadas:")
        for etapa in etapas:
            print(f"- {etapa}")

# Fun√ß√£o para agendar a execu√ß√£o
def agendar_execucao():
    schedule.every(30).minutes.do(executar_rotina)


if __name__ == '__main__':
    # Iniciar o agendamento apenas quando executado como script
    agendar_execucao()

    # Executar a rotina uma vez antes de agendar
    executar_rotina()

    while True:
        agora = datetime.now().hour
        if 8 <= agora < 22:
            schedule.run_pending()  # Executa as tarefas agendadas
            if not schedule.get_jobs():  # Verifica se n√£o h√° tarefas agendadas
                print("Aguardando a pr√≥xima execu√ß√£o...")
        time.sleep(30)  # Aguarda 30 segundos antes de verificar novamente