# üèóÔ∏è Arquitetura e API - Refer√™ncia T√©cnica

**Para:** Arquitetos, Tech Leads, Code Reviewers  
**N√≠vel:** Avan√ßado  
**Tempo de leitura:** 30-45 min

---

## üìã √çndice

1. [Vis√£o Geral da Arquitetura](#vis√£o-geral-da-arquitetura)
2. [Fluxo de Dados](#fluxo-de-dados)
3. [Fun√ß√µes Principais](#fun√ß√µes-principais)
4. [Configura√ß√µes](#configura√ß√µes)
5. [Estrutura de Dados](#estrutura-de-dados)
6. [Integra√ß√£o SQL Server](#integra√ß√£o-sql-server)

---

## Vis√£o Geral da Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           SISTEMA CORPORATIVO (Web)                  ‚îÇ
‚îÇ           https://neo.solucoes.plus/                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ Selenium (Chrome/Edge)
                   ‚îú‚îÄ Login com 2FA/OTP
                   ‚îú‚îÄ Navega√ß√£o
                   ‚îî‚îÄ Download Excel
                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           ROBO DOWNLOAD NEO (Python)                 ‚îÇ
‚îÇ           ‚îú‚îÄ app.py (main)                           ‚îÇ
‚îÇ           ‚îú‚îÄ Automa√ß√£o Web (Selenium)                ‚îÇ
‚îÇ           ‚îú‚îÄ Parse Excel (Pandas)                    ‚îÇ
‚îÇ           ‚îî‚îÄ Envio SQL (Pyodbc)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ         ‚îÇ         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇExcel ‚îÇ  ‚îÇConfigs ‚îÇ  ‚îÇSQL Server ‚îÇ
    ‚îÇFiles ‚îÇ  ‚îÇ .env   ‚îÇ  ‚îÇ   ODBC    ‚îÇ
    ‚îÇ      ‚îÇ  ‚îÇJSON    ‚îÇ  ‚îÇ           ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ         ‚îÇ         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ[downloads/]
                   [logs/]
```

---

## Fluxo de Dados

### 1. Inicializa√ß√£o
```
app.py in√≠cio
  ‚îú‚îÄ Carregar .env
  ‚îú‚îÄ Validar vari√°veis obrigat√≥rias
  ‚îú‚îÄ Carregar map_relative.json (XPaths)
  ‚îú‚îÄ Criar pastas (downloads, logs, screenshots)
  ‚îî‚îÄ Iniciar logger
```

### 2. Login
```
Selenium (Chrome/Edge)
  ‚îú‚îÄ Acessar SYS_URL
  ‚îú‚îÄ Preencher username/password
  ‚îú‚îÄ Gerar OTP (POST a OTP_URL)
  ‚îú‚îÄ Confirmar OTP
  ‚îú‚îÄ Aguardar redirecionamento
  ‚îî‚îÄ Ready para downloads
```

### 3. Download (3 tipos)
```
Para cada relat√≥rio:
  1. Navegar para painel
  2. Selecionar data (D-90 ou D-92)
  3. Clicar "Exportar"
  4. Aguardar arquivo pronto
  5. Download via cookies de sess√£o
  6. Salvar em ./downloads/
```

### 4. Parse Excel
```
Arquivo baixado
  ‚îú‚îÄ Ler com pandas.read_excel()
  ‚îú‚îÄ Normalizar headers (sem acentos)
  ‚îú‚îÄ Mapear colunas com nocodb_map.json
  ‚îú‚îÄ Formatar datas (%Y-%m-%d %H:%M:%S)
  ‚îú‚îÄ Converter tipos (int, float, string)
  ‚îî‚îÄ Retornar lista de dicts
```

### 5. Envio SQL Server
```
Lista de registros
  ‚îú‚îÄ Conectar ao SQL (ODBC)
  ‚îú‚îÄ Dividir em batches (25 registros)
  ‚îú‚îÄ Para cada batch:
  ‚îÇ   ‚îú‚îÄ Preparar INSERT statement
  ‚îÇ   ‚îú‚îÄ PROCESSAR CADA REGISTRO INDIVIDUALMENTE (Novo!)
  ‚îÇ   ‚îÇ   ‚îú‚îÄ Try: cursor.execute(insert_stmt, values)
  ‚îÇ   ‚îÇ   ‚îú‚îÄ Except IntegrityError:
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ Se PRIMARY KEY ‚Üí Marcar como duplicata (ignora, n√£o retry)
  ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ batch_duplicate_count += 1
  ‚îÇ   ‚îÇ   ‚îî‚îÄ Except OutroErro:
  ‚îÇ   ‚îÇ       ‚îî‚îÄ batch_error_count += 1
  ‚îÇ   ‚îú‚îÄ Retry com backoff exponencial (1.5^attempt) APENAS para erros n√£o-integridade
  ‚îÇ   ‚îú‚îÄ Contar: batch_success_count, batch_duplicate_count, batch_error_count
  ‚îÇ   ‚îú‚îÄ Log em JSONL com contadores (Novo!)
  ‚îÇ   ‚îî‚îÄ Commit (ap√≥s processar TODOS os registros da batch)
  ‚îî‚îÄ Registrar resumo com taxa de sucesso (%)
```

---

## Fun√ß√µes Principais

### 1. Conex√£o SQL

**`get_mssql_connection() -> Connection`**

Abre conex√£o ODBC ao SQL Server.

```python
def get_mssql_connection():
    """Cria conex√£o com SQL Server usando credenciais do .env"""
    try:
        db_server = os.getenv('DB_SERVER')           # 'servidor,porta'
        db_database = os.getenv('DB_DATABASE')       # 'rpa_neocrm'
        db_username = os.getenv('DB_USERNAME')       # 'usuario'
        db_password = os.getenv('DB_PASSWORD')       # 'senha'
        db_driver = os.getenv('DB_DRIVER')           # 'ODBC Driver 18'
        
        conn_string = (
            f'Driver={{{db_driver}}};'
            f'Server={db_server};'
            f'Database={db_database};'
            f'UID={db_username};'
            f'PWD={db_password};'
            f'Encrypt=no;TrustServerCertificate=no;Connection Timeout=30;'
        )
        connection = pyodbc.connect(conn_string)
        logger.info(f"Conex√£o OK: {db_server}/{db_database}")
        return connection
    except Exception as e:
        logger.error(f"Erro ao conectar: {e}")
        raise
```

**Par√¢metros:**
- Nenhum (l√™ do `.env`)

**Retorna:**
- `pyodbc.Connection` - Conex√£o ativa

**Levanta:**
- `Exception` - Se conex√£o falhar

**Exemplo:**
```python
conn = get_mssql_connection()
cursor = conn.cursor()
# ... usar
cursor.close()
conn.close()
```

---

### 2. Envio de Registros

**`post_records_to_mssql(records, table_name='producao', file_name=None) -> dict`**

Envia registros em batches para SQL Server com retry autom√°tico.

```python
def post_records_to_mssql(records, table_name='producao', file_name=None):
    """
    Envia registros para SQL Server em batches.
    
    Args:
        records (list): Lista de dicts com dados
        table_name (str): Nome da tabela para logs ('producao', 'atividades', 'atividades_status')
        file_name (str): Nome do arquivo (para determinar tabela via sql_map.json)
    
    Returns:
        dict: {'success': N, 'failed': N, 'total': N, 'batches_total': N, 'batches_failed': N}
    
    Respeta:
        - DRY_RUN: Se true, apenas loga (n√£o envia)
        - BATCH_SIZE: Tamanho de cada lote (padr√£o: 25)
        - POST_RETRIES: Tentativas por batch (padr√£o: 3)
        - BACKOFF_BASE: Base exponencial (padr√£o: 1.5)
    """
```

**Fluxo:**

```
1. Carregar sql_map.json
   ‚îú‚îÄ Encontrar tabela (por file_name)
   ‚îú‚îÄ Obter nome da tabela SQL
   ‚îî‚îÄ Obter colunas esperadas

2. Processar registros
   ‚îú‚îÄ Converter None ‚Üí ""
   ‚îú‚îÄ Converter datetime ‚Üí "YYYY-MM-DD HH:MM:SS"
   ‚îú‚îÄ Converter float/int ‚Üí string
   ‚îî‚îÄ Garantir todas as colunas existem

3. Dividir em batches (25 registros cada)

4. Para cada batch:
   ‚îú‚îÄ Se DRY_RUN=true:
   ‚îÇ   ‚îî‚îÄ Log em JSONL com status='DRY_RUN'
   ‚îú‚îÄ Sen√£o:
   ‚îÇ   ‚îú‚îÄ Conectar ao SQL
   ‚îÇ   ‚îú‚îÄ Preparar INSERT parametrizado
   ‚îÇ   ‚îú‚îÄ Executar cursor.execute() N vezes
   ‚îÇ   ‚îú‚îÄ Commit
   ‚îÇ   ‚îú‚îÄ Log em JSONL com status='sent'
   ‚îÇ   ‚îî‚îÄ Retry com backoff se falhar

5. Retornar resumo
```

**Sa√≠da de Erro (Retry):**

```
Tentativa 1 (erro real): Falha ‚Üí aguarda 1.5^1 = 1.5s
Tentativa 2 (erro real): Falha ‚Üí aguarda 1.5^2 = 2.25s
Tentativa 3 (erro real): Falha ‚Üí marca como 'failed'

Tentativa 1 (PRIMARY KEY duplicata): Falha ‚Üí N√ÉO RETRY (ignorada)
```

**Per-Record Processing (Novo - Fase 6):**

```python
# Antes: Falha em 1 duplicata = toda batch falha (0/25 salvos)
# Depois: Falha em 1 duplicata = batch continua (24/25 salvos)

batch_success_count = 0
batch_duplicate_count = 0
batch_error_count = 0

for idx, record in enumerate(batch, 1):
    try:
        cursor.execute(insert_stmt, values)
        batch_success_count += 1
    except pyodbc.IntegrityError as ie:
        batch_duplicate_count += 1
        logger.debug(f"‚ö†Ô∏è  DUPLICATA DETECTADA: {chave_primaria}")
        # N√ÉO tenta retry (n√£o vai resolver)
    except Exception as record_error:
        batch_error_count += 1
        logger.warning(f"Erro: {type(record_error).__name__}")

# Resultado esperado: "24 inseridos, 1 duplicata ignorada, 0 erros"
```

**Logging em `sent_records_producao.jsonl` (Novo formato - Fase 6):**

```jsonl
{"status":"sent","table":"EXPORTACAO_PRODUCAO","batch_num":2541,"batch_success_count":24,"batch_duplicate_count":1,"batch_error_count":0,"record":{...},"timestamp":"2025-10-28 15:30:45"}
{"status":"sent","table":"EXPORTACAO_PRODUCAO","batch_num":2542,"batch_success_count":25,"batch_duplicate_count":0,"batch_error_count":0,"record":{...},"timestamp":"2025-10-28 15:30:46"}
{"status":"failed","table":"EXPORTACAO_PRODUCAO","batch_num":2543,"batch_success_count":20,"batch_duplicate_count":0,"batch_error_count":5,"error":"Connection timeout","timestamp":"2025-10-28 15:30:47"}
{"status":"DRY_RUN","payload":{...}}
```

**Resumo de Envio (sa√≠da em console - Novo formato):**

```
‚úÖ [producao] Batch 2541: 24 inseridos, 1 duplicata ignorada, 0 erros
‚úÖ [producao] Batch 2542: 25 inseridos, 0 duplicatas, 0 erros
‚ùå [producao] Batch 2543: 20 inseridos, 0 duplicatas, 5 erros
üìä ESTAT√çSTICAS: Taxa de sucesso de registros: 98.2% | Batches bem-sucedidos: 2504/2549 (98.2%)
‚úÖ Detalhes: 62577 inseridos, 1175 duplicatas ignoradas, 0 erros
‚ö†Ô∏è ATEN√á√ÉO: 45 batch(es) falharam com 1125 registros. Verifique logs/sent_records_atividades_status.jsonl
```

---

### 3. Parse de Arquivo

**`parse_export_producao(file_path) -> list`**

Parse flex√≠vel de Excel com mapeamento.

```python
def parse_export_producao(file_path):
    """
    Parse Excel com normaliza√ß√£o flex√≠vel.
    
    Args:
        file_path (str): Caminho para arquivo .xlsx
    
    Returns:
        list: Lista de dicts, uma por linha
    
    L√™:
        - nocodb_map.json para headers esperados
        - file_path via pandas.read_excel()
    
    Processa:
        - Normaliza headers (remove acentos, pontua√ß√£o)
        - Match tolerante (substring)
        - Converte datas (DD/MM/YYYY ‚Üí YYYY-MM-DD HH:MM:SS)
        - Concatena campos extras em 'TAGS'
    """
```

**Exemplo:**

```python
# Entrada: ExportacaoProducao.xlsx
# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# ‚îÇ Data    ‚îÇ Cliente  ‚îÇ Valor  ‚îÇ
# ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
# ‚îÇ 28/10.. ‚îÇ XYZ Ltda ‚îÇ 1500   ‚îÇ
# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

records = parse_export_producao('./downloads/ExportacaoProducao.xlsx')

# Retorna:
# [
#   {
#     'DATA': '2025-10-28 14:30:00',
#     'CLIENTE': 'XYZ Ltda',
#     'VALOR': '1500',
#     'TAGS': ''
#   }
# ]
```

---

### 4. Limpeza de Arquivos Antigos

**`limpar_arquivos_antigos_downloads(palavras=None, padroes_regex=None, extensoes=(".xlsx",)) -> int`**

Remove arquivos antigos antes de cada download (evita conflitos).

```python
def limpar_arquivos_antigos_downloads(...):
    """
    Remove arquivos da pasta DOWNLOADS_DIR que combinem com padr√µes.
    
    Args:
        palavras (list): Palavras-chave para buscar (ex: 'exportacao', 'producao')
        padroes_regex (list): Padr√µes regex (ex: [r'^exportacao.*'])
        extensoes (tuple): Extens√µes alvo (ex: ('.xlsx',))
    
    Returns:
        int: Quantidade de arquivos removidos
    
    Normaliza nomes (sem acentos, lowercase) e compara com padr√µes.
    """
```

---

## Configura√ß√µes

### `.env` - Vari√°veis Obrigat√≥rias

```properties
# Sistema
SYS_URL                    # URL do sistema corporativo
SYS_USERNAME               # Usu√°rio do sistema
SYS_PASSWORD               # Senha do sistema
SYS_SECRET_OTP             # Token base32 para OTP
OTP_URL                    # URL do servidor de OTP

# Navegador
BROWSER                    # 'chrome' ou 'edge'
HEADLESS                   # 'true' ou 'false'
RETRIES_DOWNLOAD           # Tentativas (default: 3)
TIMEOUT_DOWNLOAD           # Segundos (default: 60)

# Downloads
DOWNLOADS_DIR              # Pasta local (default: ./downloads)

# SQL Server (OBRIGAT√ìRIO!)
DB_SERVER                  # 'servidor,porta'
DB_DATABASE                # 'nome_banco'
DB_USERNAME                # 'usuario_sql'
DB_PASSWORD                # 'senha_sql'
DB_DRIVER                  # 'ODBC Driver 18 for SQL Server'

# Envio
BATCH_SIZE                 # Registros por batch (default: 25)
POST_RETRIES               # Tentativas por batch (default: 3)
BACKOFF_BASE               # Base exponencial (default: 1.5)
DRY_RUN                    # 'true' ou 'false' (default: false)
```

### üìÅ `\bases\` - Pasta de Configura√ß√£o (Novo - Fase 4)

**IMPORTANTE:** Todos os arquivos JSON de configura√ß√£o **DEVEM estar em `\bases\`**

**Estrutura esperada:**

```
robo_download_neo/
‚îú‚îÄ‚îÄ bases/                         ‚Üê NOVA PASTA
‚îÇ   ‚îú‚îÄ‚îÄ map_relative.json         ‚Üê Carregado daqui (OBRIGAT√ìRIO)
‚îÇ   ‚îú‚îÄ‚îÄ nocodb_map.json           ‚Üê Carregado daqui (OBRIGAT√ìRIO)
‚îÇ   ‚îú‚îÄ‚îÄ sql_map.json              ‚Üê Carregado daqui (OBRIGAT√ìRIO)
‚îú‚îÄ‚îÄ downloads/
‚îú‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ app.py
‚îî‚îÄ‚îÄ ...
```

**Migra√ß√£o (se upgrading de vers√£o antiga):**
```bash
# 1. Criar pasta
mkdir bases

# 2. Mover JSONs
move map_relative.json bases/
move nocodb_map.json bases/
move sql_map.json bases/

# 3. Validar (app.py vai falhar se JSONs n√£o encontrados - erro claro)
python app.py
```

### `map_relative.json` - XPaths

```json
{
  "login": {
    "username_field": "//input[@type='text' and @name='username']",
    "password_field": "//input[@type='password']",
    "otp_field": "//input[@placeholder='C√≥digo']",
    "login_button": "//button[contains(text(), 'Entrar')]"
  },
  "producao": {
    "panel": "//div[@data-tab='producao']",
    "date_picker": "//input[@name='dataInicio']",
    "search_button": "//button[@type='submit']",
    "download_link": "//a[contains(@href, 'exportacao')]"
  }
}
```

### `nocodb_map.json` - Mapeamento Excel

```json
{
  "ExportacaoProducao.xlsx": [
    "DATA", "CLIENTE", "PRODUTO", "QUANTIDADE", "VALOR", "PEDIDO_ID", "STATUS", "TAGS"
  ],
  "Exportacao Atividade.xlsx": [
    "DATA", "TIPO", "DESCRICAO", "RESPONSAVEL", "STATUS", "TAGS"
  ],
  "Exportacao Status.xlsx": [
    "DATA", "ATIVIDADE_ID", "STATUS_ANTERIOR", "STATUS_NOVO", "MOTIVO", "TAGS"
  ]
}
```

### `sql_map.json` - Mapeamento SQL

```json
{
  "ExportacaoProducao.xlsx": {
    "tabela": "EXPORTACAO_PRODUCAO",
    "colunas": ["DATA", "CLIENTE", "PRODUTO", "QUANTIDADE", "VALOR", ...]
  },
  "Exportacao Atividade.xlsx": {
    "tabela": "EXPORTACAO_ATIVIDADE",
    "colunas": ["DATA", "TIPO", "DESCRICAO", ...]
  },
  "Exportacao Status.xlsx": {
    "tabela": "EXPORTACAO_STATUS",
    "colunas": ["DATA", "ATIVIDADE_ID", "STATUS_ANTERIOR", ...]
  }
}
```

---

## Estrutura de Dados

### Dados de Entrada (Excel)

```
Headers normalizados por nocodb_map.json:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DATA ‚îÇ CLIENTE ‚îÇ PRODUTO ‚îÇ QUANTIDADE   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 28/10/25 ‚îÇ ABC Ltd ‚îÇ Widget A ‚îÇ 100     ‚îÇ
‚îÇ 28/10/25 ‚îÇ XYZ Inc ‚îÇ Widget B ‚îÇ 50      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Processamento

```python
records = [
    {
        'DATA': '2025-10-28 14:30:00',      # Convertido
        'CLIENTE': 'ABC Ltd',
        'PRODUTO': 'Widget A',
        'QUANTIDADE': '100',                # Convertido para string
        'VALOR': '',                        # None ‚Üí ""
        'TAGS': 'extra_info | outro_campo'
    },
    ...
]
```

### Armazenamento SQL Server

```sql
INSERT INTO EXPORTACAO_PRODUCAO (DATA, CLIENTE, PRODUTO, QUANTIDADE, VALOR, CRIADO_EM)
VALUES ('2025-10-28 14:30:00', 'ABC Ltd', 'Widget A', '100', '', GETDATE());
```

---

## Integra√ß√£o SQL Server

### Conex√£o ODBC

```python
conn_string = (
    f'Driver={{ODBC Driver 18 for SQL Server}};'
    f'Server=servidor.local,1434;'
    f'Database=rpa_neocrm;'
    f'UID=usuario;'
    f'PWD=senha;'
    f'Encrypt=no;'
    f'TrustServerCertificate=no;'
    f'Connection Timeout=30;'
)
conn = pyodbc.connect(conn_string)
```

### Insert Parametrizado

```python
# Seguro contra SQL injection
insert_stmt = "INSERT INTO [EXPORTACAO_PRODUCAO] ([DATA], [CLIENTE], [PRODUTO]) VALUES (?, ?, ?)"
cursor.execute(insert_stmt, ['2025-10-28', 'ABC Ltd', 'Widget A'])
conn.commit()
```

### √çndices Recomendados

```sql
CREATE INDEX IX_DATA ON EXPORTACAO_PRODUCAO([DATA]);
CREATE INDEX IX_CLIENTE ON EXPORTACAO_PRODUCAO([CLIENTE]);
CREATE INDEX IX_DATA ON EXPORTACAO_ATIVIDADE([DATA]);
CREATE INDEX IX_TIPO ON EXPORTACAO_ATIVIDADE([TIPO]);
CREATE INDEX IX_DATA ON EXPORTACAO_STATUS([DATA]);
CREATE INDEX IX_STATUS ON EXPORTACAO_STATUS([STATUS_NOVO]);
```

---

## Tratamento de Erros - Duplicatas (Novo - Fase 6)

### O Problema Original (Antes da Fase 6)

```
Batch com 25 registros, sendo 1 duplicata:
‚îú‚îÄ INSERT registro 1 ‚Üí OK ‚úÖ
‚îú‚îÄ INSERT registro 2 ‚Üí OK ‚úÖ
‚îú‚îÄ INSERT registro 3 ‚Üí ERRO: PRIMARY KEY duplicada ‚ùå
‚îú‚îÄ INSERT registro 4-25 ‚Üí CANCELADOS (transa√ß√£o abortada)
‚îî‚îÄ RESULTADO: 0/25 registros salvos ‚ùå
```

### A Solu√ß√£o (Depois da Fase 6)

```python
# Per-Record Processing:
for idx, record in enumerate(batch, 1):
    try:
        cursor.execute(insert_stmt, values)
        batch_success_count += 1
    except pyodbc.IntegrityError as ie:
        # Duplicata = IGNORA (n√£o vai resolver com retry)
        batch_duplicate_count += 1
        logger.debug(f"‚ö†Ô∏è  DUPLICATA: Chave (47871899, 2025-05-27 16:12:58)")
    except Exception as e:
        # Erro real = REGISTRA
        batch_error_count += 1

conn.commit()  # Ap√≥s processar TODOS

# RESULTADO: 24/25 registros salvos ‚úÖ, 1 ignorado como duplicata
```

**Quando Duplicata Ocorre:**
- ‚úÖ Mesmo NUMERO + ENTROU j√° existe no banco
- ‚úÖ Exporta√ß√£o de per√≠odo sobreposto (ex: 90 dias que incluem dia anterior)
- ‚úÖ Reexecu√ß√£o do rob√¥ (rodou 2x por acidente)

**O que o App Faz Agora:**
1. Tenta INSERT
2. Pega `pyodbc.IntegrityError` com "PRIMARY KEY"
3. **N√ÉO tenta retry** (n√£o vai resolver)
4. Registra com debug level (em logs)
5. Continua pr√≥ximo registro
6. Taxa de sucesso **n√£o cai artificialmente** para 0%

---

## Fluxo de Erro - Compara√ß√£o

### Erro Real (Retry Aplicado)

```
Tentativa 1: Connection timeout ‚Üí Aguarda 1.5s
Tentativa 2: Connection timeout ‚Üí Aguarda 2.25s
Tentativa 3: Connection timeout ‚Üí FALHA (pr√≥xima batch)
```

### Duplicata (Sem Retry)

```
Tentativa 1: PRIMARY KEY violation ‚Üí IGNORA (n√£o retry)
[Continua pr√≥ximo registro]
```

---

## Performance e Escalabilidade

### Benchmarks T√≠picos

| Opera√ß√£o | Tempo |
|----------|-------|
| Login + 2FA | 15-20s |
| Download 1 arquivo | 10-30s |
| Parse 1.000 linhas | 2-5s |
| Insert 1 batch (25 reg) | 0.5-1s |
| Execu√ß√£o total (3 arquivos) | 3-5 min |

### Limite de Escalabilidade

- **Batch size:** 25 (testado, balanceado)
- **Concurrent downloads:** 1 (sequencial, por design)
- **Registros/m√™s:** ~150k (t√≠pico)
- **Storage SQL:** ~100MB/ano (3 tabelas)

### Otimiza√ß√µes Poss√≠veis

```python
# 1. Aumentar BATCH_SIZE para 50-100
#    Risco: Maior consumo de mem√≥ria

# 2. Usar bulk insert
#    Requer: Extens√£o T-SQL

# 3. Paralelizar downloads
#    Risco: Complexidade; n√£o recomendado por agora

# 4. Comprimir logs antigos
#    Benef√≠cio: Economizar disco
```

---

## Seguran√ßa

### Prote√ß√£o contra SQL Injection
```python
# ‚úì Seguro - Parametrizado
cursor.execute("INSERT INTO T ([Col]) VALUES (?)", [user_input])

# ‚úó Inseguro - String formatting
cursor.execute(f"INSERT INTO T (Col) VALUES ('{user_input}')")
```

### Credenciais
```python
# ‚úì Seguro - Em .env
db_password = os.getenv('DB_PASSWORD')

# ‚úó Inseguro - Hardcoded
db_password = "Ctelecom2017"
```

### Logs
```python
# ‚úì Seguro - Sem exposi√ß√£o
logger.info("Enviando batch para EXPORTACAO_PRODUCAO")

# ‚úó Inseguro - Exp√µe credencial
logger.info(f"Conectando como {db_username}:{db_password}")
```

---

## Testes Unit√°rios

### Testar Conex√£o
```python
import pyodbc
conn = get_mssql_connection()
assert conn is not None
conn.close()
```

### Testar Parse
```python
records = parse_export_producao('tests/fixtures/sample.xlsx')
assert len(records) > 0
assert 'DATA' in records[0]
```

### Testar Retry
```python
# Simular falha
with mock.patch('pyodbc.connect', side_effect=[Exception(), Exception(), Mock()]):
    stats = post_records_to_mssql([{'data': 'test'}])
    assert stats['success'] > 0  # Passou na 3¬™ tentativa
```

---

## Extens√µes Futuras

- [ ] Suporte a PostgreSQL
- [ ] Webhooks para notifica√ß√µes
- [ ] UI Web para monitoramento
- [ ] Backup autom√°tico
- [ ] Compress√£o de logs
- [ ] Suporte multi-arquivo simult√¢neo

---

**√öltima atualiza√ß√£o:** Outubro 2025  
**Vers√£o:** 1.0  
**Autor:** ViFigueiredo
